{"cells":[{"cell_type":"markdown","metadata":{"id":"1tjAs094VrFJ"},"source":["## CPU or GPU"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4857,"status":"ok","timestamp":1723799530054,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"},"user_tz":420},"id":"VWv1IqIRVl5y","outputId":"136b8cd8-8d35-4f7a-d8c0-896baa5e1546"},"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cpu\n"]}],"source":["import torch\n","USE_GPU = True\n","device = torch.device('cuda') if USE_GPU and torch.cuda.is_available() else torch.device('cpu')\n","print('using device:', device)"]},{"cell_type":"markdown","metadata":{"id":"eYJC4c54ngn0"},"source":["## utils.py"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6W06da6vQH_n","executionInfo":{"status":"ok","timestamp":1723799686495,"user_tz":420,"elapsed":356,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"}}},"outputs":[],"source":["import torch\n","from torch.utils import data\n","import numpy as np\n","import sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","\n","def set_seed(seed=42, device=torch.device(\"cpu\")):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if device.type == \"cuda\":\n","        torch.cuda.manual_seed(seed)\n","\n","def force_float(X_numpy):\n","    return torch.from_numpy(X_numpy.astype(np.float32))\n","\n","def convert_to_torch_loaders(Xd, Zd, Yd, batch_size):\n","    if type(Xd) != dict and type(Zd) != dict and type(Yd) != dict:\n","        Xd = {\"train\": Xd}\n","        Zd = {\"train\": Zd}\n","        Yd = {\"train\": Yd}\n","\n","    data_loaders = {}\n","    for k in Xd:\n","        X_inputs = force_float(Xd[k])\n","        Z_inputs = force_float(Zd[k])\n","        targets = force_float(Yd[k])\n","        dataset = data.TensorDataset(X_inputs, Z_inputs, targets)\n","        data_loaders[k] = data.DataLoader(dataset, batch_size, shuffle=(k == \"train\"))\n","\n","    return data_loaders\n","\n","def get_continuous_cols(X, unique_threshold = 10):\n","    continuous_cols = []\n","    for i in range(X.shape[1]):\n","        if np.issubdtype(X[:, i].dtype, np.number):\n","            unique_values = np.unique(X[:, i][~np.isnan(X[:, i])])\n","            if len(unique_values) > unique_threshold:\n","                continuous_cols.append(i)\n","    return continuous_cols\n","\n","def standardize_data(data, continuous_cols):\n","    if continuous_cols:\n","        scaler = StandardScaler() # StandardScaler objects\n","        scaler.fit(data[\"train\"][:, continuous_cols])\n","        for k in data:\n","            data[k][:, continuous_cols] = scaler.transform(data[k][:, continuous_cols])\n","    return data\n","\n","\n","def preprocess_data(\n","    X,\n","    Z,\n","    Y,\n","    valid_size=500,\n","    test_size=500,\n","    std_scale=False,\n","    unique_threshold = 10, # used to identify discrete variables\n","    batch_size=100,\n","):\n","\n","    n = X.shape[0]\n","\n","    ## Make dataset splits\n","    ntrain, nval, ntest = n - valid_size - test_size, valid_size, test_size\n","\n","    Xd = {\n","        \"train\": X[:ntrain],\n","        \"val\": X[ntrain : ntrain + nval],\n","        \"test\": X[ntrain + nval : ntrain + nval + ntest],\n","    }\n","    Zd = {\n","        \"train\": Z[:ntrain],\n","        \"val\": Z[ntrain : ntrain + nval],\n","        \"test\": Z[ntrain + nval : ntrain + nval + ntest],\n","    }\n","\n","    Yd = {\n","        \"train\": np.expand_dims(Y[:ntrain], axis=1),\n","        \"val\": np.expand_dims(Y[ntrain : ntrain + nval], axis=1),\n","        \"test\": np.expand_dims(Y[ntrain + nval : ntrain + nval + ntest], axis=1),\n","    }\n","\n","    # If the std_scale is TRUE, find continuous columns to standardize\n","    if std_scale:\n","        X_continuous_cols = get_continuous_cols(X, unique_threshold)\n","        Z_continuous_cols = get_continuous_cols(Z, unique_threshold)\n","        Y_continuous_cols = [0] if np.issubdtype(Y.dtype, np.number) and len(np.unique(Y)) > unique_threshold else []\n","        Xd = standardize_data(Xd, X_continuous_cols)\n","        Zd = standardize_data(Zd, Z_continuous_cols)\n","        Yd = standardize_data(Yd, Y_continuous_cols)\n","\n","    return convert_to_torch_loaders(Xd, Zd, Yd, batch_size)\n","\n","\n","def get_auc(interactions, ground_truth):\n","    strengths = []\n","    gt_binary_list = []\n","    for inter, strength in interactions:\n","        strengths.append(strength)\n","        if any(inter == gt for gt in ground_truth):\n","            gt_binary_list.append(1)\n","        else:\n","            gt_binary_list.append(0)\n","\n","    auc = roc_auc_score(gt_binary_list, strengths)\n","    return auc\n","\n","def print_rankings(interactions, top_k=10, spacing=14):\n","    print(\n","        justify([\"Pairwise interactions\"], spacing)\n","    )\n","    for i in range(top_k):\n","        p_inter, p_strength = interactions[i]\n","        print(\n","            justify(\n","                [\n","                    p_inter,\n","                    \"{0:.4f}\".format(p_strength),\n","                    \"\"\n","                ],\n","                spacing,\n","            )\n","        )\n","\n","def justify(row, spacing=14):\n","    return \"\".join(str(item).ljust(spacing) for item in row)\n"]},{"cell_type":"markdown","metadata":{"id":"Kfy6Qh934-X9"},"source":["## TwinterNet"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"PhkV_ou16w92","executionInfo":{"status":"ok","timestamp":1723799690065,"user_tz":420,"elapsed":862,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","import re\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","\n","class Twinter_Net(nn.Module):\n","    def __init__(\n","        self,\n","        X_num_features,\n","        Z_num_features,\n","        X_hidden_units,\n","        Z_hidden_units,\n","        X_Z_pairs_repeats,\n","        X_Z_hidden_units,\n","        X_Z_pairwise = True,\n","        X_Z_parallel = True,\n","        X_allZ_layer = True,\n","        Z_allX_layer = True,\n","        task_type = \"regression\"\n","    ):\n","        super(Twinter_Net, self).__init__()\n","\n","        if not X_Z_pairwise and not X_allZ_layer and not Z_allX_layer:\n","            raise ValueError(\"When X_Z_pairwise is False, at least one of X_allZ_layer or Z_allX_layer must be True. \"\n","                              \"You have three options:\"\n","                              \"\\nOption 1: Set both X_allZ_layer and Z_allX_layer to True;\"\n","                              \"\\nOption 2: Set X_allZ_layer to True and Z_allX_layer to False;\"\n","                              \"\\nOption 3: Set X_allZ_layer to False and Z_allX_layer to True.\")\n","\n","        self.X_num_features = X_num_features\n","        self.Z_num_features = Z_num_features\n","        self.X_Z_pairwise = X_Z_pairwise\n","        self.X_Z_parallel = X_Z_parallel\n","        self.X_allZ_layer = X_allZ_layer\n","        self.Z_allX_layer = Z_allX_layer\n","        self.X_Z_pairs_repeats = X_Z_pairs_repeats\n","        self.task_type = task_type\n","\n","        # create the X net\n","        self.X_mlp = create_mlp([X_num_features] + X_hidden_units + [1])\n","        # create the Z net\n","        self.Z_mlp = create_mlp([Z_num_features] + Z_hidden_units + [1])\n","        # create the X_Z net\n","        X_Z_pairs = (X_num_features * Z_num_features if X_Z_pairwise else\n","                   X_num_features + Z_num_features if X_allZ_layer and Z_allX_layer else\n","                   X_num_features if X_allZ_layer else\n","                   Z_num_features if Z_allX_layer else 0)\n","        X_Z_layer_units = X_Z_pairs * X_Z_pairs_repeats\n","        self.X_Z_layer = nn.Linear(X_num_features + Z_num_features, X_Z_layer_units)\n","        self.X_Z_mask = self.create_mask(X_num_features, Z_num_features)\n","        with torch.no_grad():\n","            self.X_Z_layer.weight.mul_(self.X_Z_mask)\n","        self.X_Z_relu = nn.ReLU()\n","        if X_Z_parallel:\n","            self.X_Z_parallel_mlp = self.create_X_Z_nets(X_Z_pairs, X_Z_hidden_units)\n","        else:\n","            self.X_Z_mlp = create_mlp([X_Z_layer_units] + X_Z_hidden_units + [1])\n","\n","    def forward(self, x, z):\n","        output_X = self.X_mlp(x)\n","        output_Z = self.Z_mlp(z)\n","        x_z = torch.cat((x, z), dim=1)\n","        x_z_layer = self.X_Z_layer(x_z)\n","        x_z_layer = self.X_Z_relu(self.X_Z_layer(x_z))\n","        if self.X_Z_parallel:\n","            output_X_Z = self.forward_X_Z_nets(x_z_layer, self.X_Z_parallel_mlp)\n","        else:\n","            output_X_Z = self.X_Z_mlp(x_z_layer)\n","        output_sum = output_X + output_Z + output_X_Z\n","        if self.task_type == \"regression\":\n","            return output_sum\n","        elif self.task_type == \"classification\":\n","            return torch.sigmoid(output_sum)\n","\n","    def create_X_Z_nets(self, X_Z_pairs, X_Z_hidden_units):\n","        x_z_mlp_list = [\n","            create_mlp([self.X_Z_pairs_repeats] + X_Z_hidden_units + [1], out_bias=False)\n","            for i in range(X_Z_pairs)\n","        ]\n","\n","        if self.X_Z_pairwise:\n","            for j in range(self.X_num_features):\n","                for k in range(self.Z_num_features):\n","                    setattr(self, \"X\" + str(j) + \"_Z\" + str(k) + \"_mlp\", x_z_mlp_list[j * self.Z_num_features + k])\n","        else:\n","            if self.X_allZ_layer and self.Z_allX_layer:\n","                for j in range(self.X_num_features):\n","                    setattr(self, \"X\" + str(j) + \"_Z\" + \"_mlp\", x_z_mlp_list[j])\n","                for k in range(self.Z_num_features):\n","                    setattr(self, \"X\" + \"_Z\" + str(k) + \"_mlp\", x_z_mlp_list[self.X_num_features + k])\n","            elif self.X_allZ_layer:\n","                for j in range(self.X_num_features):\n","                    setattr(self, \"X\" + str(j) + \"_Z\" + \"_mlp\", x_z_mlp_list[j])\n","            elif self.Z_allX_layer:\n","                for k in range(self.Z_num_features):\n","                    setattr(self, \"X\" + \"_Z\" + str(k) + \"_mlp\", x_z_mlp_list[k])\n","\n","        return x_z_mlp_list\n","\n","    def forward_X_Z_nets(self, x_z_layer, mlps):\n","        forwarded_x_z_mlps = []\n","        for i, mlp in enumerate(mlps):\n","            x_z_layer_selected_columns = slice(i*self.X_Z_pairs_repeats, (i+1)*self.X_Z_pairs_repeats)\n","            forwarded_x_z_mlps.append(mlp(x_z_layer[:, x_z_layer_selected_columns]))\n","        forwarded_x_z_mlp = sum(forwarded_x_z_mlps)\n","        return forwarded_x_z_mlp\n","\n","    def create_mask(self, p, q):\n","        if self.X_Z_pairwise:\n","            vec, identity = np.ones(q), np.eye(q)\n","            mask_x = np.zeros((p*q, p))\n","            for i in range(p):\n","                mask_x[i*q:(i+1)*q, i] = vec\n","            mask_z = np.vstack([identity] * p)\n","            mask = np.append(mask_x, mask_z, axis=1)\n","        else:\n","            if self.X_allZ_layer and self.Z_allX_layer:\n","                mask = np.block([[np.eye(p), np.ones((p, q))], [np.ones((q, p)), np.eye(q)]])\n","            elif self.X_allZ_layer:\n","                mask = np.block([np.eye(p), np.ones((p, q))])\n","            elif self.Z_allX_layer:\n","                mask = np.block([np.ones((q, p)), np.eye(q)])\n","        mask_repeated = np.repeat(mask, repeats=self.X_Z_pairs_repeats, axis=0)\n","\n","        return torch.tensor(mask_repeated, dtype=torch.float32)\n","\n","\n","def create_mlp(layer_sizes, out_bias=True):\n","    ls = list(layer_sizes)\n","    layers = nn.ModuleList()\n","    for i in range(1, len(ls) - 1):\n","        layers.append(nn.Linear(int(ls[i - 1]), int(ls[i])))\n","        layers.append(nn.ReLU())\n","    layers.append(nn.Linear(int(ls[-2]), int(ls[-1]), bias=out_bias))\n","    return nn.Sequential(*layers)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"i6qp33NQVPbJ","executionInfo":{"status":"ok","timestamp":1723799693027,"user_tz":420,"elapsed":2,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"}}},"outputs":[],"source":["def train(\n","    net,\n","    data_loaders,\n","    nepochs=100,\n","    verbose=False,\n","    early_stopping=True,\n","    patience=5,\n","    l1_const=5e-5,\n","    l2_const=0,\n","    learning_rate=1e-2,\n","    penalize_MMLP = False,\n","    opt_func=optim.Adam,\n","    device=torch.device(\"cpu\"),\n","):\n","\n","    X_num_features = net.X_num_features\n","    Z_num_features = net.Z_num_features\n","    X_Z_pairwise = net.X_Z_pairwise\n","    X_Z_parallel = net.X_Z_parallel\n","    X_allZ_layer = net.X_allZ_layer\n","    Z_allX_layer = net.Z_allX_layer\n","    mask = net.X_Z_mask.to(device)\n","    task_type = net.task_type\n","\n","    optimizer = opt_func(net.parameters(), lr=learning_rate, weight_decay=l2_const)\n","    criterion = nn.MSELoss(reduction=\"mean\") if task_type == \"regression\" else nn.BCELoss(reduction=\"mean\")\n","\n","    def evaluate_loss(net, data_loader, criterion, device):\n","        losses = []\n","        for X_inputs, Z_inputs, targets in data_loader:\n","            X_inputs = X_inputs.to(device)\n","            Z_inputs = Z_inputs.to(device)\n","            targets = targets.to(device)\n","            loss = criterion(net(X_inputs, Z_inputs), targets).cpu().data\n","            losses.append(loss)\n","        return torch.stack(losses).mean()\n","\n","    def evaluate_accu(net, data_loader, device):\n","        accus = []\n","        for X_inputs, Z_inputs, targets in data_loader:\n","            X_inputs = X_inputs.to(device)\n","            Z_inputs = Z_inputs.to(device)\n","            targets = targets.to(device)\n","            outputs = net(X_inputs, Z_inputs)\n","            accu = accuracy_score(targets.squeeze(1).detach().numpy(), outputs.squeeze(1).detach().numpy().round())\n","            accus.append(accu)\n","        return np.mean(accus)\n","\n","    def evaluate_auc(net, data_loader, device):\n","        aucs = []\n","        for X_inputs, Z_inputs, targets in data_loader:\n","            X_inputs = X_inputs.to(device)\n","            Z_inputs = Z_inputs.to(device)\n","            targets = targets.to(device)\n","            outputs = net(X_inputs, Z_inputs)\n","            auc = roc_auc_score(targets.squeeze(1).detach().cpu().numpy(), outputs.squeeze(1).detach().numpy())\n","            aucs.append(auc)\n","        return np.mean(aucs)\n","\n","    best_loss = float(\"inf\")\n","    best_net = None\n","\n","    if \"val\" not in data_loaders:\n","        early_stopping = False\n","\n","    patience_counter = 0\n","\n","    if verbose:\n","        print(\"starting to train\")\n","        if early_stopping:\n","            print(\"early stopping enabled\")\n","\n","    for epoch in range(nepochs):\n","        running_loss = 0.0\n","        run_count = 0\n","        for i, data in enumerate(data_loaders[\"train\"], 0):\n","            X_inputs, Z_inputs, targets = data\n","            X_inputs = X_inputs.to(device)\n","            Z_inputs = Z_inputs.to(device)\n","            targets = targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = net(X_inputs, Z_inputs)\n","            loss = criterion(outputs, targets).mean()\n","\n","            reg_loss = 0\n","            if not X_Z_pairwise or not X_Z_parallel or penalize_MMLP:\n","                for name, param in net.named_parameters():\n","                    if name == \"X_Z_layer.weight\" and not X_Z_pairwise:\n","                        if X_allZ_layer and Z_allX_layer:\n","                            reg_loss += (torch.sum(torch.abs(param[:X_num_features, X_num_features:])) + torch.sum(torch.abs(param[X_num_features:, :X_num_features])))\n","                        elif X_allZ_layer:\n","                            reg_loss += torch.sum(torch.abs(param[:, X_num_features:]))\n","                        elif Z_allX_layer:\n","                            reg_loss += torch.sum(torch.abs(param[:, :X_num_features]))\n","                    if (\"X_Z_mlp\" in name and \"weight\" in name) and not X_Z_parallel:\n","                        reg_loss += torch.sum(torch.abs(param))\n","                    if (re.match(r\"^X_mlp\\.\\d+\\.weight$\", name) or re.match(r\"^Z_mlp\\.\\d+\\.weight$\", name)) and penalize_MMLP:\n","                        reg_loss += torch.sum(torch.abs(param))\n","\n","            (loss + reg_loss * l1_const).backward()\n","            # mask gradients for the X_Z_layer\n","            with torch.no_grad():\n","                net.X_Z_layer.weight.grad.mul_(mask)\n","            optimizer.step()\n","            running_loss += loss.item()\n","            run_count += 1\n","\n","        if epoch % 1 == 0:\n","            key = \"val\" if \"val\" in data_loaders else \"train\"\n","            val_loss = evaluate_loss(net, data_loaders[key], criterion, device)\n","\n","            if epoch % 2 == 0:\n","                if verbose:\n","                    print(\n","                        \"[epoch %d, total %d] train loss: %.4f, val loss: %.4f\"\n","                        % (epoch + 1, nepochs, running_loss / run_count, val_loss)\n","                    )\n","            if early_stopping:\n","                if val_loss < best_loss:\n","                    best_loss = val_loss\n","                    best_net = copy.deepcopy(net)\n","                    patience_counter = 0\n","                else:\n","                    patience_counter += 1\n","                    if patience_counter > patience:\n","                        net = best_net\n","                        val_loss = best_loss\n","                        if verbose:\n","                            print(\"early stopping!\")\n","                        break\n","\n","            prev_loss = running_loss\n","            running_loss = 0.0\n","\n","    if \"test\" in data_loaders:\n","        key = \"test\"\n","    elif \"val\" in data_loaders:\n","        key = \"val\"\n","    else:\n","        key = \"train\"\n","\n","    if task_type == \"regression\":\n","        test_loss = evaluate_loss(net, data_loaders[key], criterion, device).item()\n","        output = (net, test_loss)\n","        if verbose:\n","            print(\"Finished Training. Test loss: \", test_loss)\n","    elif task_type == \"classification\":\n","        test_loss = evaluate_loss(net, data_loaders[key], criterion, device).item()\n","        test_accu = evaluate_accu(net, data_loaders[key], device)\n","        test_auc = evaluate_auc(net, data_loaders[key], device)\n","        output = (net, test_loss, test_accu, test_auc)\n","        if verbose:\n","            print(\"Finished Training. Test loss: %.4f, Test accuracy: %.4f, Test auc: %.4f\" % (test_loss, test_accu, test_auc))\n","\n","    return output"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"diM42yzRwOSH","executionInfo":{"status":"ok","timestamp":1723799694878,"user_tz":420,"elapsed":3,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"}}},"outputs":[],"source":["import bisect\n","import operator\n","import numpy as np\n","import torch\n","from torch.utils import data\n","import re\n","from collections import defaultdict\n","\n","def get_weights(model):\n","\n","    X_Z_mlp_weights = defaultdict(list)\n","\n","    for name, param in model.named_parameters():\n","        if \"weight\" not in name:\n","            continue\n","        if \"X_Z_layer\" in name:\n","            X_Z_layer_weights = param.cpu().detach().numpy()\n","        else:\n","            # when X_Z_parallel = False\n","            match_X_Z = re.search(r\"X_Z_mlp\", name)\n","            # when X_Z_parallel = True\n","            match_Xj_Zk = re.search(r\"X\\d+_Z\\d+_mlp\", name) # X_Z_pairwise = True\n","            match_Xj_Z = re.search(r\"X\\d+_Z+_mlp\", name) # X_Z_pairwise = False, X_allZ_layer = True, Z_allX_layer = False\n","            match_X_Zk = re.search(r\"X+_Z\\d+_mlp\", name) # X_Z_pairwise = False, X_allZ_layer = False, Z_allX_layer = True\n","            if match_Xj_Zk:\n","                X_Z_mlp_weights[match_Xj_Zk.group(0)].append(param.cpu().detach().numpy())\n","            elif match_Xj_Z:\n","                X_Z_mlp_weights[match_Xj_Z.group(0)].append(param.cpu().detach().numpy())\n","            elif match_X_Zk:\n","                X_Z_mlp_weights[match_X_Zk.group(0)].append(param.cpu().detach().numpy())\n","            elif match_X_Z:\n","                X_Z_mlp_weights[match_X_Z.group(0)].append(param.cpu().detach().numpy())\n","\n","    return X_Z_layer_weights, X_Z_mlp_weights\n","\n","def preprocess_weights(weights):\n","    X_Z_input_weights, X_Z_later_weights = weights\n","    w_input = np.abs(X_Z_input_weights)\n","    w_later = {}\n","    for name in X_Z_later_weights:\n","        mlp_weights = X_Z_later_weights[name]\n","        later_weights = np.abs(mlp_weights[-1])\n","        for i in range(len(mlp_weights) - 2, -1, -1):\n","            later_weights = np.matmul(later_weights, np.abs(mlp_weights[i]))\n","        w_later[name] = later_weights\n","\n","    return w_input, w_later\n","\n","def interpret_interactions(w_input, w_later, X_num_features, Z_num_features, X_Z_incoming, X_allZ_layer, Z_allX_layer, X_Z_pairs_repeats):\n","\n","    w_later_list = []\n","    for name, value in w_later.items():\n","        if len(value.shape) == 2:\n","            value = value.flatten()\n","        w_later_list.extend(value.tolist())\n","\n","    X_Z_pairwise = len(w_later_list) == X_num_features * Z_num_features * X_Z_pairs_repeats\n","    X_w_input, Z_w_input = w_input[:, :X_num_features], w_input[:, X_num_features:]\n","    x_w_index, z_w_index = np.arange(X_num_features), np.arange(Z_num_features)\n","\n","    if X_Z_pairwise:\n","        X_index, Z_index = np.repeat(x_w_index, Z_num_features * X_Z_pairs_repeats), np.tile(np.repeat(z_w_index, X_Z_pairs_repeats), X_num_features)\n","        row_index = np.arange(w_input.shape[0])\n","    else:\n","        X_index_part1, X_index_part2 = np.repeat(x_w_index, Z_num_features * X_Z_pairs_repeats), np.tile(x_w_index, Z_num_features * X_Z_pairs_repeats)\n","        Z_index_part1, Z_index_part2 = np.tile(z_w_index, X_num_features * X_Z_pairs_repeats), np.repeat(z_w_index, X_num_features * X_Z_pairs_repeats)\n","        row_index_part1, row_index_part2 = np.repeat(np.arange(X_num_features * X_Z_pairs_repeats), Z_num_features), np.repeat(np.arange(Z_num_features * X_Z_pairs_repeats), X_num_features)\n","\n","        if X_allZ_layer and Z_allX_layer:\n","            X_index = np.concatenate((X_index_part1, X_index_part2))\n","            Z_index = np.concatenate((Z_index_part1, Z_index_part2))\n","            row_index = np.concatenate((row_index_part1, row_index_part2 + X_num_features * X_Z_pairs_repeats))\n","            w_later_list1, w_later_list2 = np.repeat(w_later_list[:X_num_features*X_Z_pairs_repeats], Z_num_features), np.repeat(w_later_list[X_num_features*X_Z_pairs_repeats:], X_num_features)\n","            w_later_list = np.concatenate((w_later_list1, w_later_list2))\n","        elif X_allZ_layer:\n","            X_index, Z_index, row_index = X_index_part1, Z_index_part1, row_index_part1\n","            w_later_list = np.repeat(w_later_list, Z_num_features)\n","        elif Z_allX_layer:\n","            X_index, Z_index, row_index = X_index_part2, Z_index_part2, row_index_part2\n","            w_later_list = np.repeat(w_later_list, X_num_features)\n","\n","    if X_Z_incoming == \"mean\":\n","        strength = np.mean([X_w_input[row_index, X_index], Z_w_input[row_index, Z_index]], axis=0) * w_later_list\n","    elif X_Z_incoming == \"min\":\n","        strength = np.min([X_w_input[row_index, X_index], Z_w_input[row_index, Z_index]], axis=0) * w_later_list\n","    interaction_strength = list(zip(zip(X_index, Z_index), strength))\n","\n","    if X_Z_pairwise:\n","        interaction_ranking = interaction_strength\n","    else:\n","        interaction_ranking = defaultdict(int)\n","        for i in range(len(interaction_strength)):\n","            name = (X_index[i], Z_index[i])\n","            value = strength[i]\n","            interaction_ranking[name] += value\n","        interaction_ranking = [(name, strength) for name, strength in interaction_ranking.items()]\n","\n","    interaction_ranking.sort(key=lambda x: x[1], reverse=True)\n","\n","    return interaction_ranking\n","\n","def make_one_indexed(interaction_ranking):\n","    return [(tuple(np.array(i) + 1), s) for i, s in interaction_ranking]\n","\n","def get_interactions(weights, X_num_features, Z_num_features, X_Z_pairs_repeats, X_Z_incoming = \"min\", X_allZ_layer = True, Z_allX_layer = True, one_indexed=False):\n","\n","    w_input, w_later = preprocess_weights(weights)\n","\n","    interaction_ranking = interpret_interactions(w_input, w_later, X_num_features, Z_num_features, X_Z_incoming, X_allZ_layer, Z_allX_layer, X_Z_pairs_repeats)\n","\n","    if one_indexed:\n","        return make_one_indexed(interaction_ranking)\n","    else:\n","        return interaction_ranking"]},{"cell_type":"markdown","source":["# Synthtic function"],"metadata":{"id":"hqJPj7KfeXTK"}},{"cell_type":"code","source":["def synth_asym_func1_ver1(X, Z, task_type):\n","\n","    # Extract individual features from X and Z matrices\n","    X1, X2, X3, X4, X5  = X[:,0], X[:,1], X[:,2], X[:,3], X[:,4]\n","    X6, X7, X8, X9, X10 = X[:,5], X[:,6], X[:,7], X[:,8], X[:,9]\n","    X11, X12, X13, X14, X15 = X[:,10], X[:,11], X[:,12], X[:,13], X[:,14]\n","    Z1, Z2, Z3, Z4, Z5 = Z[:,0], Z[:,1], Z[:,2], Z[:,3], Z[:,4]\n","    Z6, Z7, Z8, Z9, Z10 = Z[:,5], Z[:,6], Z[:,7], Z[:,8], Z[:,9]\n","    Z11, Z12, Z13, Z14, Z15 = Z[:,10], Z[:,11], Z[:,12], Z[:,13], Z[:,14]\n","\n","    # Define two-view interactions between X and Z\n","    interaction1 = + X1 * Z1\n","    interaction2 = - X2 * Z1\n","    interaction3 = + X3 * Z1\n","    interaction4 = - X4 * Z2\n","    interaction5 = + X4 * Z3\n","    interaction6 = - X4 * Z4\n","    interaction7 = + X5 * Z5\n","    interaction8 = - X6 * Z5\n","    interaction9 = + X7 * Z6\n","    interaction10 = - X8 * Z7\n","    interaction11 = + X8 * Z8\n","    interaction12 = - X9 * Z9\n","    interaction13 = + X9 * Z10\n","    interaction14 = - X10 * Z11\n","    interaction15 = + X11 * Z11\n","    interaction16 = - X12 * Z12\n","    interaction17 = + X13 * Z12\n","\n","    # Define within-view effects for X and Z\n","    X_within_effects = X1 - 2*X2 + X3 - X4 * X5 + X6 - X7 + X8 - X9 + X10 * X11 - X12 + X13 - X14 + X15 - X10 * X13 + X14 * X15\n","    Z_within_effects = Z1 - Z2 + Z3 - Z4 + 2*Z5 - Z6 * Z7 + Z8 - Z9 + Z10 - Z11 + Z12 - Z13 + Z14 - Z15 + Z13 * Z14 - Z3 * Z15 + Z14 * Z15\n","\n","    # Genereate the linear output\n","    linear_output = (\n","        interaction1 + interaction2 + interaction3 + interaction4 + interaction5 +\n","        interaction6 + interaction7 + interaction8 + interaction9 + interaction10 +\n","        interaction11 + interaction12 + interaction13 + interaction14 + interaction15 +\n","        interaction16 + interaction17 + X_within_effects + Z_within_effects\n","    )\n","\n","    # Define the ground truth for within-view interactions\n","    ground_truth = [ (1,1), (2,1), (3,1), (4,2), (4,3), (4,4), (5,5), (6,5), (7,6), (8,7), (8,8), (9,9), (9,10), (10,11), (11,11), (12,12), (13,12)]\n","\n","    # Generate the response variable Y based on the task type (regression or classification)\n","    if task_type == \"regression\":\n","        Y = linear_output\n","    elif task_type == \"classification\":\n","        Y = generate_binary_response(linear_output)\n","\n","    return Y, ground_truth\n"],"metadata":{"id":"f3CV0tG7eS4K","executionInfo":{"status":"ok","timestamp":1723800323451,"user_tz":420,"elapsed":815,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Running Example"],"metadata":{"id":"k-6U4NENebyj"}},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223601,"status":"ok","timestamp":1723801135049,"user":{"displayName":"Hai Lu","userId":"14068106626829288025"},"user_tz":420},"id":"lkwUPko6Lm_f","outputId":"91b6a5b9-1ee3-41d9-db0f-f44111414004"},"outputs":[{"output_type":"stream","name":"stdout","text":["starting to train\n","early stopping enabled\n","[epoch 1, total 100] train loss: 0.6389, val loss: 0.0981\n","[epoch 3, total 100] train loss: 0.0283, val loss: 0.0347\n","[epoch 5, total 100] train loss: 0.0155, val loss: 0.0240\n","[epoch 7, total 100] train loss: 0.0111, val loss: 0.0160\n","[epoch 9, total 100] train loss: 0.0107, val loss: 0.0151\n","[epoch 11, total 100] train loss: 0.0082, val loss: 0.0148\n","[epoch 13, total 100] train loss: 0.0092, val loss: 0.0127\n","[epoch 15, total 100] train loss: 0.0086, val loss: 0.0145\n","[epoch 17, total 100] train loss: 0.0126, val loss: 0.0151\n","[epoch 19, total 100] train loss: 0.0075, val loss: 0.0107\n","[epoch 21, total 100] train loss: 0.0121, val loss: 0.0101\n","[epoch 23, total 100] train loss: 0.0111, val loss: 0.0160\n","[epoch 25, total 100] train loss: 0.0075, val loss: 0.0108\n","[epoch 27, total 100] train loss: 0.0065, val loss: 0.0182\n","[epoch 29, total 100] train loss: 0.0067, val loss: 0.0136\n","[epoch 31, total 100] train loss: 0.0080, val loss: 0.0099\n","[epoch 33, total 100] train loss: 0.0073, val loss: 0.0095\n","[epoch 35, total 100] train loss: 0.0045, val loss: 0.0085\n","[epoch 37, total 100] train loss: 0.0101, val loss: 0.0095\n","[epoch 39, total 100] train loss: 0.0098, val loss: 0.0108\n","early stopping!\n","Finished Training. Test loss:  0.007811307441443205\n","AUC: 1.0\n"]}],"source":["import numpy as np\n","\n","# Set the number of features for X and Z\n","X_num_features, Z_num_features = 100, 20\n","\n","# Generate random data for X and Z\n","n = 10000\n","X = np.random.rand(n, X_num_features)\n","Z = np.random.rand(n, Z_num_features)\n","\n","# Generate the synthetic response variable Y and ground truth interactions using synth_asym_func1_ver1\n","task_type = \"regression\"\n","Y, ground_truth = synth_asym_func1_ver1(X, Z, task_type)\n","\n","# Set the sizes for test, validation, and batch\n","valid_size = 1125\n","test_size = 2500\n","batch_size=100\n","\n","# Preprocess the data\n","data_loaders = preprocess_data(X, Z, Y, valid_size=valid_size, test_size=test_size, batch_size=batch_size, std_scale=True)\n","\n","# Initialize the TwinterNet model\n","model = Twinter_Net(\n","    X_num_features, Z_num_features,\n","    X_hidden_units = [30, 10, 5],\n","    Z_hidden_units = [8, 5, 3],\n","    X_Z_pairs_repeats=10,\n","    X_Z_hidden_units=[10, 10],\n","    X_Z_pairwise=False,\n","    X_Z_parallel=True,\n","    X_allZ_layer=True,\n","    Z_allX_layer=False\n",").to(device)\n","\n","# Train the model\n","model, loss = train(model, data_loaders, verbose=True)\n","\n","# Detect interactions from the model weights\n","model_weights = get_weights(model)\n","interactions = get_interactions(\n","    model_weights, X_num_features, Z_num_features, X_Z_incoming=\"min\",\n","    X_Z_pairs_repeats=10, X_allZ_layer=True, Z_allX_layer=False, one_indexed=True\n",")\n","\n","# Evaluate the model by calculating AUC\n","auc = get_auc(interactions, ground_truth)\n","print(\"AUC:\", auc)\n"]}],"metadata":{"colab":{"collapsed_sections":["eYJC4c54ngn0","Kfy6Qh934-X9","9P-tZ7pZrhBW","OClJamdb2yT1","cLfBD-RdwwCk","_WIqbMZ9HDw_"],"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}